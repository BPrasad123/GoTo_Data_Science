


**Nested Cross Validation**  
https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9

**Alternate to Gradient Descent**  
Ans1: Simulated Annealing  
https://stats.stackexchange.com/questions/97014/what-are-alternatives-of-gradient-descent  
Many other:  
http://www.seas.ucla.edu/~vandenbe/ee236c.html

**Avoiding local minima**  
Use gaussian process  
https://towardsdatascience.com/deep-neural-networks-vs-gaussian-processes-similarities-differences-and-trade-offs-18647376d799


**Gradient descent**  

https://www.cs.ubc.ca/~schmidtm/Courses/340-F17/L15.pdf


**Regularization**  
https://rubikscode.net/2020/10/12/back-to-machine-learning-basics-regularization/


**Hyperparameter tuning**  
Tree-structured Parzen Estimators (TPE) is the state-of-the-art for Bayesian optimization algorithms. 

Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is the state-of-the-art for evolutionary optimization algorithms. 

Comparing the two, TPE tends to find a good set of hyperparameters with a smaller number of iterations, but CMA-ES tends to be better in actually finding the global optimum. 

https://www.advian.fi/en/blog/best-tools-and-some-practices-for-machine-learning



**Class weights**  



**Algorithms and techniques used for optimization**  

https://stats.stackexchange.com/questions/488194/how-gradient-descent-is-used-for-classification-with-decision-trees



Blogs to check:  
https://neptune.ai/blog/improving-machine-learning-deep-learning-models  
https://www.advian.fi/en/blog/best-tools-and-some-practices-for-machine-learning  
https://orchardbirds.github.io/bokbokbok/tutorials/log_cosh_loss.html  
