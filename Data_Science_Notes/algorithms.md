## Linear Regression ##
- [Linear Regression](#linear-regression)
- [Boosting Algorithm](#boosting-algorithm)
- [Gradient descent](#gradient-descent)
- [Clustering](#clustering)
- [Weibull](#weibull)
- **Linear Regression**  

  sklearn linear regression uses SVD for linear regression. If you want to use gradient descent, then use SGDRegressor. [Link](https://machinelearningcompass.com/machine_learning_models/linear_regression/)

- **Why Lasso helps in feature selection, and Ridge not**

  Mathematic explanation by PennState [Link](https://online.stat.psu.edu/stat508/lesson/5/5.4#:~:text=The%20lasso%20performs%20shrinkage%20so,axis%20is%20shrunk%20to%20zero.)  
  Intuitive explanation by Kent Munthe Caspersen: [Link](https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models)  
  Good summary by Analytixlabs: [Link](https://www.analytixlabs.co.in/blog/regularization-in-machine-learning/)

## Boosting Algorithm ##
- **Overview**  
  [Link](https://stackabuse.com/gradient-boosting-classifiers-in-python-with-scikit-learn/)
- **Gradient boosting from scratch**  
  [Link](https://blog.mlreview.com/gradient-boosting-from-scratch-1e317ae4587d)
- **XGBOOST**  
  [Link](https://neptune.ai/blog/xgboost-everything-you-need-to-know)

## Gradient descent ##
- Sub gradient descent [Link](https://machinelearningcompass.com/machine_learning_math/subgradient_descent/)

## Clustering ##
- **Hierarchical Clustering**  
Linkages in hierarchical clustering - Single, Complete and Average. [Link](https://www.linkedin.com/feed/update/urn:li:activity:6901750720182276096)

## Weibull ##
- **Weibull for failure analysis**  
  Blog by SAP: [Link](https://blogs.sap.com/2020/11/17/new-weibull-based-pof-curves-in-sap-predictive-asset-insights/)


